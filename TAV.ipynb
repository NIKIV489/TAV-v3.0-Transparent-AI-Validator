{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6bcce-e893-474e-8d8b-4d2a039f71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  TAV v3.0 â€” Transparent AI Validator\n",
    "#  Universal AutoML + Per-Model Feature Selection\n",
    "# ============================================================\n",
    "\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings (\"ignore\")\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  CUSTOM CSS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "st.set_page_config (\n",
    "    page_title = \"TAV v3.0 Â· Universal AutoML\",\n",
    "    page_icon = \"âš¡\",\n",
    "    layout = \"wide\",\n",
    "    initial_sidebar_state = \"expanded\",\n",
    ")\n",
    "\n",
    "st.markdown (\"\"\"\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Syne:wght@400;700;800&display=swap');\n",
    "\n",
    "html, body, [class*=\"css\"] {\n",
    "    font-family: 'Syne', sans-serif;\n",
    "    background: #0a0a0f;\n",
    "    color: #e2e8f0;\n",
    "}\n",
    "\n",
    ".stApp {\n",
    "    background: linear-gradient(135deg, #0a0a0f 0%, #0d1117 50%, #0a0f1a 100%);\n",
    "}\n",
    "\n",
    ".tav-header {\n",
    "    background: linear-gradient(90deg, #00f5a0 0%, #00d9f5 50%, #7c3aed 100%);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "    font-family: 'Syne', sans-serif;\n",
    "    font-size: 3.2rem;\n",
    "    font-weight: 800;\n",
    "    letter-spacing: -1px;\n",
    "    margin-bottom: 0;\n",
    "}\n",
    "\n",
    ".tav-sub {\n",
    "    color: #64748b;\n",
    "    font-family: 'JetBrains Mono', monospace;\n",
    "    font-size: 0.85rem;\n",
    "    letter-spacing: 3px;\n",
    "    text-transform: uppercase;\n",
    "    margin-bottom: 2rem;\n",
    "}\n",
    "\n",
    ".section-title {\n",
    "    font-family: 'JetBrains Mono', monospace;\n",
    "    font-size: 0.7rem;\n",
    "    letter-spacing: 4px;\n",
    "    text-transform: uppercase;\n",
    "    color: #00f5a0;\n",
    "    margin-bottom: 0.5rem;\n",
    "    padding-top: 2rem;\n",
    "}\n",
    "\n",
    ".best-model-banner {\n",
    "    background: linear-gradient(90deg, rgba(0,245,160,0.15), rgba(0,217,245,0.15));\n",
    "    border: 1px solid rgba(0,245,160,0.4);\n",
    "    border-radius: 12px;\n",
    "    padding: 1.2rem 2rem;\n",
    "    font-family: 'Syne', sans-serif;\n",
    "    font-size: 1.2rem;\n",
    "    font-weight: 700;\n",
    "    color: #00f5a0;\n",
    "    text-align: center;\n",
    "    margin: 1rem 0;\n",
    "}\n",
    "\n",
    ".stButton > button {\n",
    "    background: linear-gradient(90deg, #00f5a0, #00d9f5);\n",
    "    color: #0a0a0f;\n",
    "    font-family: 'Syne', sans-serif;\n",
    "    font-weight: 700;\n",
    "    border: none;\n",
    "    border-radius: 8px;\n",
    "    padding: 0.6rem 2rem;\n",
    "    font-size: 0.95rem;\n",
    "    letter-spacing: 1px;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    ".stButton > button:hover {\n",
    "    opacity: 0.85;\n",
    "    transform: translateY(-1px);\n",
    "}\n",
    "\n",
    ".stDownloadButton > button {\n",
    "    background: rgba(124,58,237,0.2);\n",
    "    border: 1px solid rgba(124,58,237,0.6);\n",
    "    color: #a78bfa;\n",
    "    font-family: 'JetBrains Mono', monospace;\n",
    "    border-radius: 8px;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "div[data-testid=\"stDataFrame\"] {\n",
    "    border-radius: 10px;\n",
    "    overflow: hidden;\n",
    "}\n",
    "\n",
    ".stSelectbox label, .stMultiSelect label, .stFileUploader label {\n",
    "    color: #94a3b8;\n",
    "    font-size: 0.85rem;\n",
    "    font-family: 'JetBrains Mono', monospace;\n",
    "    letter-spacing: 1px;\n",
    "    text-transform: uppercase;\n",
    "}\n",
    "\n",
    "section[data-testid=\"stSidebar\"] {\n",
    "    background: rgba(255,255,255,0.02);\n",
    "    border-right: 1px solid rgba(255,255,255,0.06);\n",
    "}\n",
    "\n",
    ".sidebar-logo {\n",
    "    font-family: 'Syne', sans-serif;\n",
    "    font-size: 1.8rem;\n",
    "    font-weight: 800;\n",
    "    background: linear-gradient(90deg, #00f5a0, #00d9f5);\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "}\n",
    "\n",
    "hr {\n",
    "    border-color: rgba(255,255,255,0.07);\n",
    "}\n",
    "\n",
    ".stAlert {\n",
    "    background: rgba(0,245,160,0.07);\n",
    "    border-color: rgba(0,245,160,0.3);\n",
    "    border-radius: 10px;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html = True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  SIDEBAR\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with st.sidebar:\n",
    "    st.markdown ('<div class=\"sidebar-logo\">âš¡ TAV v3.0</div>', unsafe_allow_html = True)\n",
    "    st.markdown ('<div style=\"color:#64748b;font-size:0.7rem;letter-spacing:3px;\">UNIVERSAL AUTOML</div>',\n",
    "                 unsafe_allow_html = True)\n",
    "    st.markdown (\"---\")\n",
    "\n",
    "    st.markdown (\"**âš™ï¸ Configuration**\")\n",
    "    test_size = st.slider (\"Test Split Size\", 0.10, 0.40, 0.20, 0.05)\n",
    "    cv_folds = st.slider (\"Cross-Validation Folds\", 3, 10, 5)\n",
    "    apply_smote = st.toggle (\"Apply SMOTE (Imbalance Fix)\", value = True)\n",
    "\n",
    "    st.markdown (\"---\")\n",
    "    st.markdown (\"**ğŸ¯ Feature Selection**\")\n",
    "    enable_feature_selection = st.toggle (\"Enable Per-Model Feature Selection\", value = True)\n",
    "    if enable_feature_selection:\n",
    "        feature_selection_method = st.selectbox (\n",
    "            \"Selection Method\",\n",
    "            [\"Auto (Best for Each Model)\", \"Mutual Information\", \"F-Statistic\", \"Chi-Square\"]\n",
    "        )\n",
    "        n_features_pct = st.slider (\"% Features to Keep\", 20, 100, 70, 10)\n",
    "\n",
    "    st.markdown (\"---\")\n",
    "    st.markdown (\"**ğŸ¤– Models**\")\n",
    "    model_choices = st.multiselect (\n",
    "        \"Select Models to Train\",\n",
    "        [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\",\n",
    "         \"Gradient Boosting\", \"XGBoost\"],\n",
    "        default = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\",\n",
    "                   \"Gradient Boosting\", \"XGBoost\"]\n",
    "    )\n",
    "\n",
    "    st.markdown (\"---\")\n",
    "    st.markdown (\n",
    "        '<div style=\"color:#334155;font-size:0.72rem;font-family:monospace;\">v3.0 Â· Universal Classification</div>',\n",
    "        unsafe_allow_html = True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  HEADER\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "st.markdown ('<div class=\"tav-header\">TAV v3.0</div>', unsafe_allow_html = True)\n",
    "st.markdown ('<div class=\"tav-sub\">Universal AutoML + Intelligent Feature Selection</div>', unsafe_allow_html = True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  FILE UPLOAD\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "uploaded_file = st.file_uploader (\"Upload your CSV dataset\", type = [\"csv\"])\n",
    "\n",
    "if uploaded_file is None:\n",
    "    st.info (\"ğŸ‘† Upload a CSV file to begin. TAV handles numeric, categorical, mixed, and text datasets.\")\n",
    "    st.stop ()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  LOAD DATA\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_raw = pd.read_csv (uploaded_file)\n",
    "\n",
    "# â”€â”€ Dataset Preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "st.markdown ('<div class=\"section-title\">01 Â· Dataset Overview</div>', unsafe_allow_html = True)\n",
    "\n",
    "c1, c2, c3, c4 = st.columns (4)\n",
    "c1.metric (\"Rows\", f\"{df_raw.shape [0]:,}\")\n",
    "c2.metric (\"Columns\", df_raw.shape [1])\n",
    "c3.metric (\"Numeric\", len (df_raw.select_dtypes (include = np.number).columns))\n",
    "c4.metric (\"Missing %\", f\"{df_raw.isnull ().mean ().mean () * 100:.1f}%\")\n",
    "\n",
    "st.dataframe (df_raw.head (8), use_container_width = True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  TARGET SELECTION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "st.markdown ('<div class=\"section-title\">02 Â· Target Column</div>', unsafe_allow_html = True)\n",
    "target_col = st.selectbox (\"Select the TARGET (label) column\", df_raw.columns)\n",
    "\n",
    "if st.button (\"âš¡ Run TAV AutoML\"):\n",
    "\n",
    "    df = df_raw.copy ()\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #  EDA SECTION\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    st.markdown ('<div class=\"section-title\">03 Â· Exploratory Data Analysis</div>', unsafe_allow_html = True)\n",
    "\n",
    "    tab1, tab2, tab3, tab4 = st.tabs ([\"ğŸ“Š Class Distribution\", \"ğŸ”¥ Correlation Heatmap\",\n",
    "                                       \"ğŸ“‰ Missing Values\", \"ğŸ“ˆ Feature Stats\"])\n",
    "\n",
    "    with tab1:\n",
    "        fig, ax = plt.subplots (figsize = (6, 3.5), facecolor = '#0d1117')\n",
    "        ax.set_facecolor ('#0d1117')\n",
    "        counts = df [target_col].value_counts ()\n",
    "        colors = plt.cm.viridis (np.linspace (0.2, 0.8, len (counts)))\n",
    "        bars = ax.bar (counts.index.astype (str), counts.values, color = colors, width = 0.5, edgecolor = 'none')\n",
    "        for bar, val in zip (bars, counts.values):\n",
    "            ax.text (bar.get_x () + bar.get_width () / 2, bar.get_height () + max (counts) * 0.01,\n",
    "                     f'{val:,}', ha = 'center', color = '#e2e8f0', fontsize = 10, fontweight = 'bold')\n",
    "        ax.set_xlabel ('Class', color = '#64748b')\n",
    "        ax.set_ylabel ('Count', color = '#64748b')\n",
    "        ax.set_title ('Target Class Distribution', color = '#e2e8f0', pad = 12)\n",
    "        ax.tick_params (colors = '#64748b')\n",
    "        for spine in ax.spines.values ():\n",
    "            spine.set_edgecolor ('#1e293b')\n",
    "        st.pyplot (fig, use_container_width = True)\n",
    "\n",
    "    with tab2:\n",
    "        num_df = df.select_dtypes (include = np.number)\n",
    "        if len (num_df.columns) > 1:\n",
    "            fig, ax = plt.subplots (figsize = (max (6, len (num_df.columns)), max (5, len (num_df.columns) - 1)),\n",
    "                                    facecolor = '#0d1117')\n",
    "            ax.set_facecolor ('#0d1117')\n",
    "            corr = num_df.corr ()\n",
    "            mask = np.triu (np.ones_like (corr, dtype = bool))\n",
    "            cmap = sns.diverging_palette (150, 275, s = 80, l = 55, as_cmap = True)\n",
    "            sns.heatmap (corr, mask = mask, cmap = cmap, center = 0, linewidths = 0.5,\n",
    "                         linecolor = '#0a0a0f', annot = True, fmt = '.2f',\n",
    "                         annot_kws = {'size': 8, 'color': '#e2e8f0'}, ax = ax,\n",
    "                         cbar_kws = {'shrink': 0.7})\n",
    "            ax.tick_params (colors = '#94a3b8', labelsize = 8)\n",
    "            ax.set_title (\"Correlation Matrix (Numeric Features)\", color = '#e2e8f0', pad = 12)\n",
    "            st.pyplot (fig, use_container_width = True)\n",
    "        else:\n",
    "            st.info (\"Not enough numeric columns for correlation heatmap.\")\n",
    "\n",
    "    with tab3:\n",
    "        missing = df.isnull ().sum ()\n",
    "        missing = missing [missing > 0]\n",
    "        if len (missing) == 0:\n",
    "            st.success (\"âœ… No missing values found!\")\n",
    "        else:\n",
    "            fig, ax = plt.subplots (figsize = (7, max (3, len (missing) * 0.5)), facecolor = '#0d1117')\n",
    "            ax.set_facecolor ('#0d1117')\n",
    "            pct = (missing / len (df) * 100).sort_values (ascending = True)\n",
    "            colors_m = ['#ef4444' if p > 30 else '#f59e0b' if p > 10 else '#00f5a0' for p in pct.values]\n",
    "            ax.barh (pct.index, pct.values, color = colors_m, edgecolor = 'none', height = 0.6)\n",
    "            ax.set_xlabel ('Missing %', color = '#64748b')\n",
    "            ax.set_title ('Missing Values by Feature', color = '#e2e8f0', pad = 12)\n",
    "            ax.tick_params (colors = '#94a3b8', labelsize = 9)\n",
    "            for spine in ax.spines.values ():\n",
    "                spine.set_edgecolor ('#1e293b')\n",
    "            st.pyplot (fig, use_container_width = True)\n",
    "\n",
    "    with tab4:\n",
    "        num_cols_desc = df.select_dtypes (include = np.number).columns\n",
    "        if len (num_cols_desc) > 0:\n",
    "            st.markdown (\"**Numeric Features:**\")\n",
    "            num_desc = df [num_cols_desc].describe ().T\n",
    "            st.dataframe (num_desc.style.format (\"{:.3f}\"), use_container_width = True)\n",
    "\n",
    "        cat_cols_eda = df.select_dtypes (exclude = np.number).columns\n",
    "        if len (cat_cols_eda) > 0:\n",
    "            st.markdown (\"**Categorical Features:**\")\n",
    "            for col in cat_cols_eda:\n",
    "                if col != target_col:\n",
    "                    with st.expander (f\"ğŸ“‹ {col}\"):\n",
    "                        vc = df [col].value_counts ().head (10)\n",
    "                        st.bar_chart (vc)\n",
    "\n",
    "        if len (num_cols_desc) == 0 and len (cat_cols_eda) == 0:\n",
    "            st.info (\"No features to display (only target column).\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #  PREPROCESSING PIPELINE\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    st.markdown ('<div class=\"section-title\">04 Â· Smart Preprocessing</div>', unsafe_allow_html = True)\n",
    "\n",
    "    X = df.drop (columns = [target_col])\n",
    "    y = df [target_col].copy ()\n",
    "\n",
    "    # â”€â”€ Fill missing target values with mode â”€â”€\n",
    "    if y.isnull ().any ():\n",
    "        fill_val = y.mode () [0]\n",
    "        y = y.fillna (fill_val)\n",
    "        st.info (f\"â„¹ï¸  Missing target filled with: '{fill_val}'\")\n",
    "\n",
    "    # â”€â”€ LabelEncoder for target â”€â”€\n",
    "    target_le = LabelEncoder ()\n",
    "    y = pd.Series (target_le.fit_transform (y.astype (str)), name = target_col)\n",
    "    class_mapping = {cls: int (idx) for idx, cls in enumerate (target_le.classes_)}\n",
    "    st.info (f\"ğŸ·ï¸  Target encoded â†’ {class_mapping}\")\n",
    "\n",
    "    # Identify column types\n",
    "    num_cols = X.select_dtypes (include = np.number).columns.tolist ()\n",
    "    cat_cols = X.select_dtypes (include = 'object').columns.tolist ()\n",
    "\n",
    "    # Detect binary vs nominal\n",
    "    binary_cat = [c for c in cat_cols if X [c].nunique () == 2]\n",
    "    nominal_cat = [c for c in cat_cols if X [c].nunique () > 2]\n",
    "\n",
    "    st.markdown (f\"\"\"\n",
    "    | Type | Count | Columns |\n",
    "    |------|-------|---------|\n",
    "    | **Numeric** | {len (num_cols)} | {', '.join (num_cols [:5]) if num_cols else 'â€”'}{'...' if len (num_cols) > 5 else ''} |\n",
    "    | **Binary Cat** | {len (binary_cat)} | {', '.join (binary_cat [:5]) if binary_cat else 'â€”'}{'...' if len (binary_cat) > 5 else ''} |\n",
    "    | **Nominal Cat** | {len (nominal_cat)} | {', '.join (nominal_cat [:5]) if nominal_cat else 'â€”'}{'...' if len (nominal_cat) > 5 else ''} |\n",
    "    \"\"\")\n",
    "\n",
    "    # Numeric pipeline: median impute + scale\n",
    "    numeric_pipe = Pipeline ([\n",
    "        ('imputer', SimpleImputer (strategy = 'median')),\n",
    "        ('scaler', StandardScaler ())\n",
    "    ])\n",
    "\n",
    "    # Binary cat: mode impute + ordinal (0/1)\n",
    "    binary_pipe = Pipeline ([\n",
    "        ('imputer', SimpleImputer (strategy = 'most_frequent')),\n",
    "        ('encoder', OrdinalEncoder (handle_unknown = 'use_encoded_value', unknown_value = -1))\n",
    "    ])\n",
    "\n",
    "    # Nominal cat: mode impute + one-hot\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    nominal_pipe = Pipeline ([\n",
    "        ('imputer', SimpleImputer (strategy = 'most_frequent')),\n",
    "        ('encoder', OneHotEncoder (handle_unknown = 'ignore', sparse_output = False))\n",
    "    ])\n",
    "\n",
    "    transformers = []\n",
    "    if num_cols:    transformers.append (('num', numeric_pipe, num_cols))\n",
    "    if binary_cat:  transformers.append (('bin', binary_pipe, binary_cat))\n",
    "    if nominal_cat: transformers.append (('nom', nominal_pipe, nominal_cat))\n",
    "\n",
    "    preprocessor = ColumnTransformer (transformers = transformers, remainder = 'drop')\n",
    "\n",
    "    # â”€â”€ CRITICAL: Split BEFORE preprocessing â”€â”€\n",
    "    # Check if stratification is safe\n",
    "    class_counts = y.value_counts ()\n",
    "    min_class_size = class_counts.min ()\n",
    "    can_stratify = min_class_size >= 2  # Need at least 2 samples per class\n",
    "\n",
    "    if not can_stratify:\n",
    "        st.warning (\n",
    "            f\"âš ï¸  Dataset too small for stratified split ({min_class_size} samples in smallest class). Using random split.\")\n",
    "        stratify_param = None\n",
    "    else:\n",
    "        stratify_param = y\n",
    "\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split (\n",
    "        X, y, test_size = test_size, random_state = 42, stratify = stratify_param\n",
    "    )\n",
    "\n",
    "    # Fit preprocessor on training data only\n",
    "    X_train = preprocessor.fit_transform (X_train_raw)\n",
    "    X_test = preprocessor.transform (X_test_raw)\n",
    "\n",
    "    st.success (f\"âœ… Preprocessing done. Training: {X_train.shape [0]} rows Ã— {X_train.shape [1]} features\")\n",
    "\n",
    "    # SMOTE\n",
    "    if apply_smote and can_stratify:  # SMOTE also needs at least 2 samples per class\n",
    "        class_counts_train = np.bincount (y_train.values)\n",
    "        imbalance_ratio = class_counts_train.min () / class_counts_train.max ()\n",
    "        if imbalance_ratio < 0.8 and class_counts_train.min () >= 2:\n",
    "            try:\n",
    "                sm = SMOTE (random_state = 42)\n",
    "                X_train, y_train = sm.fit_resample (X_train, y_train)\n",
    "                st.info (f\"âš–ï¸  SMOTE applied. Balanced training set: {X_train.shape [0]:,} samples\")\n",
    "            except ValueError as e:\n",
    "                st.warning (f\"âš ï¸  SMOTE skipped: {str (e)}\")\n",
    "        else:\n",
    "            st.info (\"ğŸ“Š Dataset reasonably balanced or too small for SMOTE.\")\n",
    "    elif apply_smote and not can_stratify:\n",
    "        st.warning (\"âš ï¸  SMOTE disabled: dataset too small (classes with only 1 sample).\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #  PER-MODEL FEATURE SELECTION\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if enable_feature_selection:\n",
    "        st.markdown ('<div class=\"section-title\">05 Â· Intelligent Feature Selection</div>', unsafe_allow_html = True)\n",
    "\n",
    "        n_features_to_keep = max (5, int (X_train.shape [1] * n_features_pct / 100))\n",
    "\n",
    "        # Define per-model selection strategies\n",
    "        selection_strategies = {\n",
    "            \"Logistic Regression\": \"f_classif\",  # Linear models like F-statistic\n",
    "            \"Decision Tree\": \"mutual_info\",  # Tree models like mutual info\n",
    "            \"Random Forest\": \"mutual_info\",\n",
    "            \"Gradient Boosting\": \"mutual_info\",\n",
    "            \"XGBoost\": \"mutual_info\"\n",
    "        }\n",
    "\n",
    "        if feature_selection_method != \"Auto (Best for Each Model)\":\n",
    "            # Override with user choice\n",
    "            method_map = {\n",
    "                \"Mutual Information\": \"mutual_info\",\n",
    "                \"F-Statistic\": \"f_classif\",\n",
    "                \"Chi-Square\": \"chi2\"\n",
    "            }\n",
    "            for k in selection_strategies:\n",
    "                selection_strategies [k] = method_map [feature_selection_method]\n",
    "\n",
    "        st.info (f\"ğŸ¯ Selecting top {n_features_pct}% features ({n_features_to_keep} features) per model\")\n",
    "\n",
    "        feature_selectors = {}\n",
    "        for model_name in model_choices:\n",
    "            strategy = selection_strategies.get (model_name, \"mutual_info\")\n",
    "\n",
    "            if strategy == \"f_classif\":\n",
    "                selector = SelectKBest (f_classif, k = min (n_features_to_keep, X_train.shape [1]))\n",
    "            elif strategy == \"chi2\":\n",
    "                # Chi2 requires non-negative features\n",
    "                selector = SelectKBest (chi2, k = min (n_features_to_keep, X_train.shape [1]))\n",
    "            else:  # mutual_info\n",
    "                selector = SelectKBest (mutual_info_classif, k = min (n_features_to_keep, X_train.shape [1]))\n",
    "\n",
    "            feature_selectors [model_name] = selector\n",
    "\n",
    "        st.success (f\"âœ… Feature selectors initialized for {len (model_choices)} models\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #  MODEL TRAINING\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    st.markdown ('<div class=\"section-title\">06 Â· Model Training & Benchmarking</div>', unsafe_allow_html = True)\n",
    "\n",
    "    ALL_MODELS = {\n",
    "        \"Logistic Regression\": LogisticRegression (max_iter = 1000, random_state = 42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier (random_state = 42),\n",
    "        \"Random Forest\": RandomForestClassifier (n_estimators = 100, random_state = 42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier (n_estimators = 100, random_state = 42),\n",
    "        \"XGBoost\": XGBClassifier (n_estimators = 100, use_label_encoder = False,\n",
    "                                  eval_metric = 'logloss', random_state = 42, verbosity = 0),\n",
    "    }\n",
    "\n",
    "    selected_models = {k: v for k, v in ALL_MODELS.items () if k in model_choices}\n",
    "\n",
    "    # Auto-detect binary vs multiclass\n",
    "    n_classes = len (np.unique (y))\n",
    "    is_binary = (n_classes == 2)\n",
    "    avg = 'binary' if is_binary else 'weighted'\n",
    "    cv_scoring = 'roc_auc' if is_binary else 'roc_auc_ovr_weighted'\n",
    "\n",
    "    if not is_binary:\n",
    "        st.info (f\"ğŸ”¢ Multiclass detected ({n_classes} classes) â€” using weighted averaging\")\n",
    "\n",
    "    results = []\n",
    "    trained = {}\n",
    "    feature_importance_data = {}\n",
    "\n",
    "    prog = st.progress (0, text = \"Training modelsâ€¦\")\n",
    "\n",
    "    # Adjust CV folds for small datasets\n",
    "    min_samples_per_fold = X_train.shape [0] // cv_folds\n",
    "    if min_samples_per_fold < n_classes:\n",
    "        safe_cv_folds = max (2, X_train.shape [0] // n_classes)\n",
    "        st.warning (f\"âš ï¸  Reducing CV folds from {cv_folds} to {safe_cv_folds} (dataset too small)\")\n",
    "        cv_folds_actual = safe_cv_folds\n",
    "    else:\n",
    "        cv_folds_actual = cv_folds\n",
    "\n",
    "    skf = StratifiedKFold (n_splits = cv_folds_actual, shuffle = True, random_state = 42) if can_stratify else None\n",
    "\n",
    "    for i, (name, model) in enumerate (selected_models.items ()):\n",
    "        prog.progress (i / len (selected_models), text = f\"Training {name}â€¦\")\n",
    "\n",
    "        # Apply feature selection if enabled\n",
    "        if enable_feature_selection:\n",
    "            selector = feature_selectors [name]\n",
    "            X_train_selected = selector.fit_transform (X_train, y_train)\n",
    "            X_test_selected = selector.transform (X_test)\n",
    "            selected_features = selector.get_support ()\n",
    "            n_selected = selected_features.sum ()\n",
    "        else:\n",
    "            X_train_selected = X_train\n",
    "            X_test_selected = X_test\n",
    "            n_selected = X_train.shape [1]\n",
    "\n",
    "        # Train model\n",
    "        model.fit (X_train_selected, y_train)\n",
    "        y_pred = model.predict (X_test_selected)\n",
    "        y_proba = model.predict_proba (X_test_selected)\n",
    "\n",
    "        # ROC-AUC calculation\n",
    "        if is_binary:\n",
    "            y_prob_for_auc = y_proba [:, 1]\n",
    "            try:\n",
    "                auc = roc_auc_score (y_test, y_prob_for_auc)\n",
    "            except ValueError as e:\n",
    "                st.warning (f\"ROC-AUC calculation failed for {name}: {e}\")\n",
    "                auc = np.nan\n",
    "        else:\n",
    "            y_prob_for_auc = y_proba\n",
    "            # Check if test set has all classes\n",
    "            unique_train_classes = np.unique (y_train)\n",
    "            unique_test_classes = np.unique (y_test)\n",
    "\n",
    "            if len (unique_test_classes) < len (unique_train_classes):\n",
    "                # Test set missing some classes - can't compute proper OVR AUC\n",
    "                st.warning (f\"âš ï¸  {name}: Test set missing some classes. Using accuracy instead of AUC.\")\n",
    "                auc = accuracy_score (y_test, y_pred)  # Fallback to accuracy\n",
    "            else:\n",
    "                try:\n",
    "                    auc = roc_auc_score (y_test, y_prob_for_auc,\n",
    "                                         multi_class = 'ovr', average = 'weighted')\n",
    "                except ValueError as e:\n",
    "                    st.warning (f\"âš ï¸  {name}: ROC-AUC failed ({str (e)}). Using accuracy.\")\n",
    "                    auc = accuracy_score (y_test, y_pred)\n",
    "\n",
    "        # Cross-validation\n",
    "        if skf is not None:\n",
    "            try:\n",
    "                cv_scores = cross_val_score (model, X_train_selected, y_train,\n",
    "                                             cv = skf, scoring = cv_scoring, n_jobs = -1)\n",
    "                cv_mean, cv_std = cv_scores.mean (), cv_scores.std ()\n",
    "            except ValueError:\n",
    "                # CV failed (too small), skip\n",
    "                cv_mean, cv_std = np.nan, np.nan\n",
    "        else:\n",
    "            # Dataset too small for CV\n",
    "            cv_mean, cv_std = np.nan, np.nan\n",
    "\n",
    "        results.append ({\n",
    "            \"Model\": name,\n",
    "            \"Features\": n_selected,\n",
    "            \"Accuracy\": accuracy_score (y_test, y_pred),\n",
    "            \"Precision\": precision_score (y_test, y_pred, average = avg, zero_division = 0),\n",
    "            \"Recall\": recall_score (y_test, y_pred, average = avg, zero_division = 0),\n",
    "            \"F1 Score\": f1_score (y_test, y_pred, average = avg, zero_division = 0),\n",
    "            \"ROC-AUC\": auc,\n",
    "            \"CV AUC Î¼\": cv_mean,\n",
    "            \"CV AUC Ïƒ\": cv_std,\n",
    "        })\n",
    "\n",
    "        if enable_feature_selection:\n",
    "            trained [name] = (model, y_proba, y_prob_for_auc, selector, selected_features)\n",
    "            feature_importance_data [name] = selected_features\n",
    "        else:\n",
    "            trained [name] = (model, y_proba, y_prob_for_auc, None, None)\n",
    "\n",
    "    prog.progress (1.0, text = \"âœ… All models trained!\")\n",
    "\n",
    "    results_df = pd.DataFrame (results).sort_values (\"ROC-AUC\", ascending = False).reset_index (drop = True)\n",
    "\n",
    "\n",
    "    # Style results table\n",
    "    def style_table (df):\n",
    "        return df.style.format ({\n",
    "            \"Features\": \"{:d}\",\n",
    "            \"Accuracy\": \"{:.4f}\", \"Precision\": \"{:.4f}\", \"Recall\": \"{:.4f}\",\n",
    "            \"F1 Score\": \"{:.4f}\", \"ROC-AUC\": \"{:.4f}\",\n",
    "            \"CV AUC Î¼\": \"{:.4f}\", \"CV AUC Ïƒ\": \"{:.4f}\",\n",
    "        }).background_gradient (subset = [\"ROC-AUC\", \"F1 Score\"], cmap = \"Greens\")\n",
    "\n",
    "\n",
    "    st.dataframe (style_table (results_df), use_container_width = True, hide_index = True)\n",
    "\n",
    "    # â”€â”€ Feature Selection Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if enable_feature_selection:\n",
    "        st.markdown ('<div class=\"section-title\">07 Â· Feature Selection Analysis</div>', unsafe_allow_html = True)\n",
    "\n",
    "        fig, ax = plt.subplots (figsize = (10, 4), facecolor = '#0d1117')\n",
    "        ax.set_facecolor ('#0d1117')\n",
    "\n",
    "        models_list = list (feature_importance_data.keys ())\n",
    "        feature_counts = [feature_importance_data [m].sum () for m in models_list]\n",
    "\n",
    "        colors_feat = ['#00f5a0', '#00d9f5', '#7c3aed', '#f59e0b', '#ef4444']\n",
    "        bars = ax.barh (models_list, feature_counts, color = colors_feat [:len (models_list)],\n",
    "                        height = 0.6, edgecolor = 'none')\n",
    "\n",
    "        for bar, val in zip (bars, feature_counts):\n",
    "            ax.text (bar.get_width () + 1, bar.get_y () + bar.get_height () / 2,\n",
    "                     f'{val}', va = 'center', color = '#e2e8f0', fontsize = 10, fontweight = 'bold')\n",
    "\n",
    "        ax.set_xlabel ('Number of Features Selected', color = '#64748b')\n",
    "        ax.set_title ('Features Selected Per Model', color = '#e2e8f0', pad = 12)\n",
    "        ax.tick_params (colors = '#94a3b8')\n",
    "        for spine in ax.spines.values ():\n",
    "            spine.set_edgecolor ('#1e293b')\n",
    "        st.pyplot (fig, use_container_width = True)\n",
    "\n",
    "    # â”€â”€ Metric Comparison Charts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    st.markdown ('<div class=\"section-title\">08 Â· Metric Comparison</div>', unsafe_allow_html = True)\n",
    "\n",
    "    metrics_to_plot = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "    fig, axes = plt.subplots (1, 5, figsize = (18, 4), facecolor = '#0d1117')\n",
    "    palette = ['#00f5a0', '#00d9f5', '#7c3aed', '#f59e0b', '#ef4444']\n",
    "\n",
    "    for ax, metric, color in zip (axes, metrics_to_plot, palette):\n",
    "        ax.set_facecolor ('#0d1117')\n",
    "        bars = ax.barh (results_df [\"Model\"], results_df [metric], color = color, alpha = 0.85,\n",
    "                        edgecolor = 'none', height = 0.55)\n",
    "        for bar, val in zip (bars, results_df [metric]):\n",
    "            ax.text (bar.get_width () - 0.01, bar.get_y () + bar.get_height () / 2,\n",
    "                     f'{val:.3f}', va = 'center', ha = 'right',\n",
    "                     color = '#0a0a0f', fontsize = 8.5, fontweight = 'bold')\n",
    "        ax.set_xlabel ('Score', color = '#64748b', fontsize = 8)\n",
    "        ax.set_title (metric, color = '#e2e8f0', fontsize = 10, pad = 8)\n",
    "        ax.tick_params (colors = '#94a3b8', labelsize = 8)\n",
    "        ax.set_xlim (0, 1.05)\n",
    "        for spine in ax.spines.values ():\n",
    "            spine.set_edgecolor ('#1e293b')\n",
    "\n",
    "    plt.tight_layout ()\n",
    "    st.pyplot (fig, use_container_width = True)\n",
    "\n",
    "    # â”€â”€ ROC Curves â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    st.markdown ('<div class=\"section-title\">09 Â· ROC Curve Comparison</div>', unsafe_allow_html = True)\n",
    "\n",
    "    fig, ax = plt.subplots (figsize = (8, 5), facecolor = '#0d1117')\n",
    "    ax.set_facecolor ('#0d1117')\n",
    "\n",
    "    roc_colors = ['#00f5a0', '#00d9f5', '#7c3aed', '#f59e0b', '#ef4444']\n",
    "    for (name, data), color in zip (trained.items (), roc_colors):\n",
    "        model, y_proba, y_prob_for_auc = data [0], data [1], data [2]\n",
    "\n",
    "        if is_binary:\n",
    "            fpr, tpr, _ = roc_curve (y_test, y_prob_for_auc)\n",
    "            auc_val = roc_auc_score (y_test, y_prob_for_auc)\n",
    "            ax.plot (fpr, tpr, label = f'{name} (AUC={auc_val:.3f})', color = color, lw = 2)\n",
    "        else:\n",
    "            from sklearn.preprocessing import label_binarize\n",
    "\n",
    "            classes = np.unique (y)\n",
    "            y_test_bin = label_binarize (y_test, classes = classes)\n",
    "            auc_val = roc_auc_score (y_test, y_prob_for_auc, multi_class = 'ovr', average = 'weighted')\n",
    "            fpr, tpr, _ = roc_curve (y_test_bin.ravel (), y_prob_for_auc.ravel ())\n",
    "            ax.plot (fpr, tpr, label = f'{name} (AUC={auc_val:.3f})', color = color, lw = 2)\n",
    "\n",
    "    ax.plot ([0, 1], [0, 1], color = '#334155', lw = 1.5, linestyle = '--', label = 'Random')\n",
    "    ax.fill_between ([0, 1], [0, 1], alpha = 0.05, color = '#334155')\n",
    "    ax.set_xlabel ('False Positive Rate', color = '#64748b')\n",
    "    ax.set_ylabel ('True Positive Rate', color = '#64748b')\n",
    "    ax.set_title ('ROC Curve â€” All Models', color = '#e2e8f0', pad = 12)\n",
    "    ax.legend (loc = 'lower right', fontsize = 8.5, framealpha = 0.15,\n",
    "               labelcolor = '#e2e8f0', facecolor = '#0d1117', edgecolor = '#1e293b')\n",
    "    ax.tick_params (colors = '#94a3b8')\n",
    "    for spine in ax.spines.values ():\n",
    "        spine.set_edgecolor ('#1e293b')\n",
    "    st.pyplot (fig, use_container_width = False)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #  BEST MODEL\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    best_name = results_df.iloc [0] [\"Model\"]\n",
    "    best_model_data = trained [best_name]\n",
    "    best_model = best_model_data [0]\n",
    "    best_selector = best_model_data [3] if enable_feature_selection else None\n",
    "    best_auc = results_df.iloc [0] [\"ROC-AUC\"]\n",
    "    best_f1 = results_df.iloc [0] [\"F1 Score\"]\n",
    "\n",
    "    st.markdown (\n",
    "        f'<div class=\"best-model-banner\">ğŸ† Best Model: {best_name} &nbsp;|&nbsp; AUC = {best_auc:.4f} &nbsp;|&nbsp; F1 = {best_f1:.4f}</div>',\n",
    "        unsafe_allow_html = True\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Confusion Matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    st.markdown ('<div class=\"section-title\">10 Â· Confusion Matrix (Best Model)</div>', unsafe_allow_html = True)\n",
    "\n",
    "    if enable_feature_selection and best_selector:\n",
    "        X_test_best = best_selector.transform (X_test)\n",
    "    else:\n",
    "        X_test_best = X_test\n",
    "\n",
    "    y_pred_best = best_model.predict (X_test_best)\n",
    "    cm = confusion_matrix (y_test, y_pred_best)\n",
    "\n",
    "    fig_size = (4.5, 3.5) if is_binary else (max (5, n_classes * 1.5), max (4, n_classes * 1.2))\n",
    "    fig, ax = plt.subplots (figsize = fig_size, facecolor = '#0d1117')\n",
    "    ax.set_facecolor ('#0d1117')\n",
    "\n",
    "    cmap_custom = sns.diverging_palette (150, 275, s = 80, l = 55, as_cmap = True)\n",
    "    class_labels = [str (c) for c in target_le.classes_]\n",
    "\n",
    "    sns.heatmap (cm, annot = True, fmt = 'd', cmap = cmap_custom,\n",
    "                 linewidths = 2, linecolor = '#0a0a0f',\n",
    "                 annot_kws = {'size': 14 if is_binary else 11, 'weight': 'bold', 'color': '#e2e8f0'},\n",
    "                 cbar = False, ax = ax,\n",
    "                 xticklabels = [f'Pred {lbl}' for lbl in class_labels],\n",
    "                 yticklabels = [f'True {lbl}' for lbl in class_labels])\n",
    "    ax.tick_params (colors = '#94a3b8', labelsize = 9)\n",
    "    ax.set_title (f'Confusion Matrix â€” {best_name}', color = '#e2e8f0', pad = 12)\n",
    "\n",
    "    if is_binary:\n",
    "        tn, fp, fn, tp = cm.ravel ()\n",
    "        col1, col2 = st.columns ([1, 2])\n",
    "        with col1:\n",
    "            st.pyplot (fig)\n",
    "        with col2:\n",
    "            st.markdown (f\"\"\"\n",
    "            | Metric | Value |\n",
    "            |--------|-------|\n",
    "            | True Positives (TP) | **{tp}** |\n",
    "            | True Negatives (TN) | **{tn}** |\n",
    "            | False Positives (FP) | **{fp}** |\n",
    "            | False Negatives (FN) | **{fn}** |\n",
    "            | Sensitivity | **{tp / (tp + fn) if (tp + fn) > 0 else 0:.4f}** |\n",
    "            | Specificity | **{tn / (tn + fp) if (tn + fp) > 0 else 0:.4f}** |\n",
    "            | Precision | **{tp / (tp + fp) if (tp + fp) > 0 else 0:.4f}** |\n",
    "            \"\"\")\n",
    "    else:\n",
    "        st.pyplot (fig)\n",
    "        class_acc = cm.diagonal () / cm.sum (axis = 1)\n",
    "        acc_df = pd.DataFrame ({\n",
    "            'Class': class_labels,\n",
    "            'Samples': cm.sum (axis = 1),\n",
    "            'Correct': cm.diagonal (),\n",
    "            'Accuracy': class_acc\n",
    "        })\n",
    "        st.markdown (\"**Per-Class Performance:**\")\n",
    "        st.dataframe (acc_df.style.format ({'Accuracy': '{:.2%}'}), use_container_width = True)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    #  EXPORT\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    st.markdown ('<div class=\"section-title\">11 Â· Export</div>', unsafe_allow_html = True)\n",
    "\n",
    "    col_d1, col_d2 = st.columns (2)\n",
    "\n",
    "    with col_d1:\n",
    "        model_bytes = io.BytesIO ()\n",
    "        export_data = {\n",
    "            'model': best_model,\n",
    "            'preprocessor': preprocessor,\n",
    "            'target_encoder': target_le,\n",
    "            'feature_selector': best_selector,\n",
    "            'feature_names': preprocessor.get_feature_names_out () if hasattr (preprocessor,\n",
    "                                                                               'get_feature_names_out') else None\n",
    "        }\n",
    "        joblib.dump (export_data, model_bytes)\n",
    "        model_bytes.seek (0)\n",
    "        st.download_button (\n",
    "            label = \"â¬‡ Download Best Model (.pkl)\",\n",
    "            data = model_bytes,\n",
    "            file_name = f\"TAV_v3_{best_name.replace (' ', '_')}_model.pkl\",\n",
    "            mime = \"application/octet-stream\"\n",
    "        )\n",
    "\n",
    "    with col_d2:\n",
    "        csv_bytes = results_df.to_csv (index = False).encode ()\n",
    "        st.download_button (\n",
    "            label = \"â¬‡ Download Metrics Report (.csv)\",\n",
    "            data = csv_bytes,\n",
    "            file_name = \"TAV_v3_model_comparison.csv\",\n",
    "            mime = \"text/csv\"\n",
    "        )\n",
    "\n",
    "    st.markdown (\"---\")\n",
    "    st.markdown (\n",
    "        '<div style=\"text-align:center;color:#334155;font-family:monospace;font-size:0.75rem;\">'\n",
    "        'TAV v3.0 Â· Universal AutoML Â· Built with Streamlit + Scikit-learn'\n",
    "        '</div>',\n",
    "        unsafe_allow_html = True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
